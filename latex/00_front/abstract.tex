\clearpage\pagestyle{empty}
\begin{SingleSpacing}
\begin{abstract}

\begin{center}
    {\large
        \theTitle \\
        \theSubtitle
        \par
        \vspace{\baselineskip}
        \theAuthor
        \par
        \vspace{\baselineskip}
    }
\end{center}

This dissertation proposes a general methodological framework for the application of computational text analysis to the study of long duration material processes of transformation, beyond their traditional application to the study of discurse and rhetorical action.
Over a thin theory of the linguistic nature of social facts, the proposed methodology revolves around the compilation of term co-occurrence matrices and their projection into different representations of an hypothetical semantic space.
These representations offer solutions to two problems inherent to social scientific research: that of ``mapping'' features in a given representation to theoretical entities and that of ``alignment'' of the features seen in models built from different sources in order to enable their comparison.

The data requirements of the exercise are discussed through the introduction of the notion of a ``narrative horizon'', the extent to which a given source incorporates a narrative account in its rendering of the context that produces it.
Useful primary data will consist of text with short narrative horizons, such that the ideal source will correspond to a continuous archive of institutional, ideally bureaucratic text produced as mere documentation of a definite population of more or less stable and comparable social facts across a couple of centuries.
Such a primary source is available in the Proceedings of the Old Bailey (POB), a collection of transcriptions of $197,752$ criminal trials seen by the Old Bailey and the Central Criminal Court of London and Middlesex between $1674$ and $1913$ that includes verbatim transcriptions of witness testimony.
The POB is used to demonstrate the proposed framework, starting with the analysis of the evolution of an historical corpus to illustrate the procedure by which provenance data is used to construct longitudinal and cross-sectional comparisons of different corpus segments.

The co-occurrence matrices obtained from the POB corpus are used to demonstrate two different projections:
semantic networks that model different notions of similarity between the terms in a corpus' lexicon as an adjacency matrix describing a graph and
semantic vector spaces are approximate a lower-dimensional representation of an hypothetical semantic space from the co-occurrence matrix which is treated as its empirical consequence.

Semantic networks are presented as discrete methematical objects equipped with operations for the construction of sets of terms over which order can be induced using any of various measures of significance of the association between a term set and the terms it contains.
This offers a solution to the mapping problem in which theoretical entities are associated to (ordered) sets.
They also offer a solution to the alignment problem in which the sets obtained from the projection of different corpus segments are compared through any of several similarity measures computed over their intersection or union.

Semantic vector spaces are presented as continous mathematical objects equipped with a semantically meaningful metric that facilitates the definition of neighbourhoods and regions of the semantic space, and in some cases, also equipped with semantically meaningful orientations that allow the tracing of dimensions across them.
This offers a solution to the mapping problem in which theoretical entities are associated to higher-order linear structures that are naturally aligned for relative comparisons, but that require the production of a common basis set for an absolute comparison.

The dissertation concludes with the proposition of a general research program for the systematic compilation of text distributional patterns in order to facilitate a much needed process of calibration required by the techniques discussed in the previous chapters.
Two specific avenues for further research are identified.
First, the development of incremental methods of projection that allow a semantic model to be updated as new observations come along, an area that has received considerable attention from the field of electronic finance and the pervasive use of Gentleman's algorithm for matrix factorization.
Second, the development of additively decomposable models that may be combined or disaggregated to obtain a similar result to the one that would have been obtained had the model being computed from the union or difference of their inputs.
This is established to be dependent on whether the functions that actualize a given model are associative under addition or not.

\end{abstract}
\end{SingleSpacing}
\clearpage\pagestyle{plain}
