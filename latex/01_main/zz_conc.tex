\chapter{Conclusion}
\label{chap:conc}

This dissertation attempted to show how computational text analysis can be used as the basis over which to construct non-narrative representations of the past.
In this sense, it is an attempt at providing a solution to the fundamental epistemological problem that is given by the reliance of our traditional way of producing historical knowledge on narrative accounts.
The epistemological problem is given by the confusion that exists between data and theory when both the descriptions of the past that we use as historical data and the appreciation of these data that we use to build theories consist on the same exercise: interpretation of the events in the past in the context of some narrative, such that one historian's theories are a future historian's data.
Since narratives provide means to adjudicate significance within them but no means to adjudicate between them, it is ultimately impossible to choose between different narrative accounts from nothing but the authority of its author.

The choice of computational text analysis to carry out this exercise is purely practical.
In principle, the core idea for a solution to the problem that motivates this dissertation is given by the construction of non-narrative, formal representations of the state of affairs at a given point in time, such that a direct comparison between the representations corresponding to adjacent points in time can be used to construct endogenous measures of significance of the transitions between them that do not depend on a narrative account.
There are different ways in which such pictures of the state of affairs can be constructed; the original idea that eventually revealed itself to be at the core of this effort was not based on computational text analysis, but on the consideration of large, natural ontologies, like the ones available from different structured information resources available in the internet, an idea that was abandoned and shelved some eight years ago out of the tremendous computational costs involved in that exercise\footnote{
    The idea was to use the network of connections between articles in the Wikipedia as a natural ontology in order to establish the significance of events in the world from the effect that their incorporation into the encyclopedia had on the global connectivity structure; i.e. the terrorist attack in Bali on October 1st, 2005 would create new connections between the regions of the encyclopedia associated to Bali and the regions associated to terrorism in a way that other events, like the death of Michael Jackson, would not.
    Then the impact of these emerging connections on the global connectivity structure can be used as a measure of significance.
    Back in 2009, the processing time necessary for merely constructing the graph corresponding to the entire Wikipedia took several months; I didn't even begin to explore how long it would take to establish the impact of incoming connections on the global structure, but from what I understand now about graph theory, this is \emph{hard}.
    And this does not take into account the data acquisition problems associated to parsing the dynamic graph of the Wikipedia from the edit history of each article, etcetera.
    I am now very interested in revisiting this idea, particularly since new resources have been made available that facilitate this task enormously, like the Freebase, an RDF encoded representation of the ontology contained in the Wikipedia that was in its very early stages back when the idea was originally formulated.
}; computational text analysis just seems like a more promising avenue because (1) there is tremendous interest in the development of better tools and the ones currently available have reached impressive levels of accuracy and sophistication and (2) because they allow to carry out this exercise retroactively from their construction using historical archives that are increasingly been made available electronically.

There are two implications to this:
First, the problem that motivates this dissertation is more general than the particular avenue for solution explored in this work.
Second, the particular avenue for solution explored in this work is also more general, to the extent that the same approach can be used to address problems beyond the ones that we obtain in historical sociology from the indeterminacy of the past.
This is the reason why I said in the introduction that the techniques discussed in this work are of interest even to an sceptic reader that is not convinced by either my formulation of the original problem or the possibility of (1) constructing pictures of states of affairs from computational text analysis or (2) the general idea of establishing measures of significance from the comparison of pictures of adjacent states of affairs.

When evaluated against that general objective, this dissertation is clearly incomplete: I have not provided convincing evidence of the way in which text analysis can capture material social processes beyond some very general outlines, and I have not provided solutions to many of the problems that arise in an effort to operationalise the proposed methodology to the level of detail that the objective actually requires.
This is given mostly by the choices I made while completing this project, which can be summarised in two guiding principles: no discretion on my part and no black boxes.
All parametric and methodological decisions taken in the production of the few results that I have been capable of reporting were taken on purely endogenous criteria, like the use of token stream coverage for lexical sampling, global connectivity thresholds for edge pruning, etc.
All of the relevant procedures used in the production of the results were implemented in software by myself, with the exception of certain specific tasks, for which I only used software that I could examine in detail (like the NLP pipeline), and all of these implementations were designed with the first principle in mind: everything that is particular to the analysis of the POB corpus was included only as parametric choices\footnote{
    E.g. I didn't write a SAX parser \emph{for the POB}, I wrote a SAX parser that can be used to process any XML document; the specific structure of the POB is given to this component as a set of options, etc.
}.
I stand firmly behind these choices, because I believe that this is the reason why this dissertation is a valuable contribution even though it has failed to fulfil its ostensible objective:
It was only through this exercise that I acquired an understanding of the current state of the art in computational text analysis that is barely sufficient to produce what I believe to be this work's main value; the formulation of a standard, consistent and fully reproducible method for the application of computational text analysis to problems that are beyond its usual application in the study of discursive phenomena.

The proposed methodology revolves around a clear separation between the different stages of analysis into three broad areas:
Text normalisation and production of a token stream and associated linguistic and non-linguistic data;
Collection of distributional patterns in the form of co-occurrence matrices;
Projection of co-occurrence matrices into higher-order mathematical structures.

If the proposed methodology has any hopes of actually being useful for the completion of the ostensible objective, it is because this strict separation between the different stages of any computational analysis application allows for the separation between data and theory that I claim to be necessary to overcome the deep epistemological problems that we find in historical sociology\footnote{
    Which I also happen to believe to be applicable to questions in \emph{general} sociology, but I have yet to formulate that argument in full.
}:
Theoretical considerations are not allowed to play any part in the production of the formal representations of $S$ that we obtain from different projections, but these formal representations offer myriad possibilities for constructing theoretical arguments from the solutions they facilitate to what I call the mapping problem: the specific exercise of associating features in $S$ to the $\sigma \in \Sigma$ that make up the conceptual repertoire of sociological theories.

I have purposefully refrained from discussing the details of this exercise beyond a basic exposition of the operations that are available as elementary building blocks in the different projections that I have discussed; it is in this exercise where the proposed method invites the deployment of sociological imagination; I expect the discussion in the previous chapters to serve mostly as fodder for ideas about the type of arguments that can be constructed by exploiting some of the richer aspects of these mathematical structures.
There is, however, one central consideration that needs to be taken into account regarding different $S \rightarrow \sigma \in \Sigma$ solutions: care should be taken when entertaining solutions that depend on the identity of terms, and every effort should be made in the direction of producing a characterisation of different entities that does not depend on the locations of specific terms in a given projection of $S$; language changes, sometimes incredibly fast\footnote{
    The best example of this phenomenon is found in \citet{hamilton2016} and their example of the change in meaning of the word \term{awful} from ``solemn, majestic'' to ``appalling, terrible'' in \emph{only 50 years}.
    The prevalent uses of ``sentiment analysis'' in the social sciences in order to say, for example, that the change in uses of adjectives like ``good'' or ``bad'' is indicative of some change in the general opinion about some abstract social entities should be regarded with extreme suspicion in light of this.
}.
This means that we should expect the meaning of words to be fluid, and this implies that whatever is pointed at by the location of a given term in one place can be expected to be different to whatever is pointed at by \emph{the same term} in a different place.
Practically, this consideration was incorporated into the methodology through the insistence in establishing a distinction between the first-order units of observation given by terms themselves and the second order units of analysis that we construct from operations over them.
This explains the interest in e.g. looking for ways around the problem of lexical overlap, entertaining the idea of using motifs over vertex sets in semantic network projections or relying on geometric structures rather than points and distances in semantic vector spaces.

Following this general recommendation, however, requires a much more systematic exploration of the internal structure of both semantic networks and semantic vector spaces, in order to be able to ascertain the significance of observed regularities and discontinuities.
I have tried to indicate some general directions for this necessary exploration.
The idea about a connection between equivalence classes and connectivity structures in semantic networks, as well as the differential patterns of association between vertex strength and cluster contribution presented in \autoref{chap:semnet}, or the appreciable connections between the local structure in vector spaces and the connectivity patterns in semantic networks as well as the relationship between the inner biases in a corpus and the organisation of the semantic vector space along dimensions reported in chapter \autoref{chap:wspaces} are examples of this.
Exploring the questions that these results suggest appear as a necessary step in order to be able to use these structures to talk about what sociologists are interested in: dimensions of differentiation, social mechanisms, association structures.
I have suggested that a view of the social that bears a direct connection to linguistic phenomena (like Searle's linguistic construction of social reality) may be useful for this, but there are surely others.

What seems clear in any case, is that this exercise requires a fundamental process of standardisation in the ways in which we carry out these analyses.
I believe this to be the secondary contribution of this dissertation, as the clear separation between the different steps in the analysis that it proposes translates into a clear partition of the parameter spaces of the different models that are usually applied in this field.
This means that issues like the choice of co-occurrence weighting functions, context definitions including granularity of segmentation and width and symmetry of co-occurring windows, etc. can be encapsulated into the process that creates values of $X$, without interference from or interfering in the process by which different values of $X$ are projected into different versions of $S$.

Full exploitation of this parameter space reduction would require two additional steps:
First, figuring out a way to store values of $X$ that allowed for more flexibility in the ways in which it is produced in order to remove some parametric restrictions and facilitate other styles of projection apart from the ones discussed in this work.
Second, producing the actual means to carry out the exercise that is proposed as inspiration for this methodology: devise ways of solving the analytical tasks that are currently solved via independent techniques as a function of the values of $X$.

In closing, it seems clear to me that the discussion in this dissertation suggests the outlines of an ambitious research program centred around the production and systematic collection of co-occurrence counts for as much text as we can get our hands on.
This is currently technically possible, and it is surely being carried out in opaque corporate research environments, like the ones that produced the geometric models that have recently revolutionised the field.
In addition to the sheer hardware costs associated to this exercise, there is one outstanding problem that was suggested in passing in the discussion of the previous chapters: the production of values of $S$ in an incremental or ``online'' way, such that they are capable of updating their value as new observations are acquired, being composed from different partial values and decomposed into partial values, etc.

I have not given any thought to this problem in the context of semantic networks, and the difficulties faced by the strategy discussed in this work do not seem surmountable, to the extent that the method for their construction that I discussed relies on some global attributes of $X$ that would change it is updated (or partitioned, joined, etc), beginning with the edge pruning process.
However, this seems much more accessible in the case of semantic vector spaces, as it is a problem that has received some attention from efforts to avoid the re-computation of the SVD that was necessary in traditional models in order to incorporate new information.
The current solution in that context is the use of random indexing, which has been shown to produce comparable results to the ones obtained from the traditional implementation of LSA and HAL, to the point that most currently available implementations of either LSA or HAL are based on random indexing and do not actually carry out an SVD step at all (e.g. \citet{widdows2010}).

Unfortunately, I am not aware of a similar solution for production of the more interesting geometric models that we are interested in.
My lack of familiarity with neural networks prevents me from knowing in advance whether word2vec and other predictive models can be computed incrementally.
GloVe and other models based on approximation of an error function against $X$ as if it were a correlation matrix should be possible to compute, update and compose incrementally; there is a largely successful industry in which this capability is the bread and butter of their analysis: electronic finance, in which models need to be updated continuously in order to incorporate incoming data from market fluctuations.
My initial research in this direction suggests that the standard method in this field would have direct application as a substitute to the stochastic gradient descent method currently used in the standard GloVe implementation: Gentleman's algorithm \citep{gentleman1974,miller1992}, a procedure designed specifically for solving linear least square problems in a way in which it allows for adding or deleting observations.
